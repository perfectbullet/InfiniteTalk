import asyncio
import subprocess
from pathlib import Path
from typing import Optional

from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import StreamingResponse, JSONResponse

from api_server.api_loger import logger
from api_server.config import config
from api_server.database import db_manager

router = APIRouter(prefix="/tasks_logs", tags=["Task Logs"])

# ===== é…ç½®å¸¸é‡ =====
LOG_BASE_DIR = config.LOG_DIR
DEFAULT_TAIL_LINES = 100
MAX_TAIL_LINES = 10000
LOG_STREAM_INTERVAL = 0.5  # ç§’
MAX_LOG_SIZE = 100 * 1024 * 1024  # 100MB


# ===== è¾…åŠ©å‡½æ•° =====
def get_log_path(task_id: str) -> Path:
    """
    è·å–ä»»åŠ¡æ—¥å¿—æ–‡ä»¶è·¯å¾„

    Args:
        task_id: ä»»åŠ¡ID

    Returns:
        Path: æ—¥å¿—æ–‡ä»¶è·¯å¾„

    Raises:
        HTTPException: å¦‚æœä»»åŠ¡IDæ— æ•ˆ
    """
    # éªŒè¯ä»»åŠ¡IDæ ¼å¼ï¼ˆé˜²æ­¢è·¯å¾„éå†æ”»å‡»ï¼‰
    if not task_id or ".." in task_id or "/" in task_id or "\\" in task_id:
        raise HTTPException(
            status_code=400,
            detail="æ— æ•ˆçš„ä»»åŠ¡IDæ ¼å¼"
        )

    log_path = LOG_BASE_DIR / f"task_{task_id}.log"

    if not log_path.exists():
        logger.warning(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
        raise HTTPException(
            status_code=404,
            detail=f"ä»»åŠ¡ {task_id} çš„æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"
        )

    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = log_path.stat().st_size
    if file_size > MAX_LOG_SIZE:
        logger.warning(f"æ—¥å¿—æ–‡ä»¶è¿‡å¤§: {file_size} bytes")

    return log_path


async def check_task_status(task_id: str) -> Optional[str]:
    """
    æ£€æŸ¥ä»»åŠ¡çŠ¶æ€

    Args:
        task_id: ä»»åŠ¡ID

    Returns:
        Optional[str]: ä»»åŠ¡çŠ¶æ€ï¼Œå¦‚æœä»»åŠ¡ä¸å­˜åœ¨åˆ™è¿”å› None
    """
    try:
        task_doc = await db_manager.get_task_by_id(task_id)
        return task_doc.status if task_doc else None
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥ [{task_id}]: {str(e)}")
        return None


def is_task_finished(status: Optional[str]) -> bool:
    """åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ"""
    return status in ['completed', 'failed', 'cancelled']


# ===== è·¯ç”±ç«¯ç‚¹ =====

@router.get(
    "/{task_id}/logs",
    summary="è·å–ä»»åŠ¡æ—¥å¿—",
    description="è·å–æŒ‡å®šä»»åŠ¡çš„æ—¥å¿—å†…å®¹ï¼Œæ”¯æŒå®æ—¶è·Ÿè¸ªæ¨¡å¼",
    response_description="è¿”å›æ—¥å¿—å†…å®¹æˆ–æ—¥å¿—æµ"
)
async def get_task_logs(
        task_id: str,
        follow: bool = Query(
            False,
            description="æ˜¯å¦å®æ—¶è·Ÿè¸ªæ—¥å¿—ï¼ˆç±»ä¼¼ `tail -f`ï¼‰"
        )
):
    """
    ## ğŸ“‹ è·å–ä»»åŠ¡æ—¥å¿—

    ### åŠŸèƒ½è¯´æ˜
    - **æ™®é€šæ¨¡å¼** (`follow=false`): ä¸€æ¬¡æ€§è¿”å›å…¨éƒ¨æ—¥å¿—å†…å®¹
    - **è·Ÿè¸ªæ¨¡å¼** (`follow=true`): å®æ—¶æµå¼è¿”å›æ—¥å¿—ï¼ŒæŒç»­ç›‘æ§æ–°å†…å®¹

    ### å‚æ•°
    - `task_id`: ä»»åŠ¡ID
    - `follow`: æ˜¯å¦å¯ç”¨è·Ÿè¸ªæ¨¡å¼

    ### è¿”å›å€¼
    - **æ™®é€šæ¨¡å¼**: JSON æ ¼å¼ï¼ŒåŒ…å«å®Œæ•´æ—¥å¿—
    - **è·Ÿè¸ªæ¨¡å¼**: æ–‡æœ¬æµ (text/event-stream)

    ### ç¤ºä¾‹
    ```bash
    # è·å–å…¨éƒ¨æ—¥å¿—
    GET /api/tasks/{task_id}/logs

    # å®æ—¶è·Ÿè¸ªæ—¥å¿—
    GET /api/tasks/{task_id}/logs?follow=true
    ```
    """
    log_path = get_log_path(task_id)
    logger.info(f"{'å®æ—¶' if follow else 'é™æ€'}è¯»å–æ—¥å¿—: {log_path}")

    if follow:
        # ===== å®æ—¶è·Ÿè¸ªæ¨¡å¼ =====
        async def log_streamer():
            """å¼‚æ­¥æ—¥å¿—æµç”Ÿæˆå™¨"""
            try:
                # æ‰“å¼€æ–‡ä»¶
                with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                    # 1. å…ˆè¯»å–å·²æœ‰å†…å®¹
                    logger.info(f"ğŸ“– å¼€å§‹è¯»å–å·²æœ‰æ—¥å¿—å†…å®¹")
                    line_count = 0

                    for line in f:
                        yield f"data: {line}\n\n"  # SSE æ ¼å¼
                        line_count += 1

                    logger.info(f"âœ… å·²è¯»å– {line_count} è¡Œå†å²æ—¥å¿—")

                    # 2. æŒç»­ç›‘æ§æ–°å†…å®¹
                    logger.info("ğŸ‘€ å¼€å§‹ç›‘æ§æ–°æ—¥å¿—...")
                    no_data_count = 0
                    max_no_data_iterations = 120  # 60ç§’æ— æ•°æ®åˆ™åœæ­¢ (120 * 0.5s)

                    while True:
                        line = f.readline()

                        if line:
                            yield f"data: {line}\n\n"
                            no_data_count = 0  # é‡ç½®è®¡æ•°å™¨
                        else:
                            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                            status = await check_task_status(task_id)

                            if is_task_finished(status):
                                logger.info(f"âœ… ä»»åŠ¡å·²å®Œæˆ (status={status})ï¼Œåœæ­¢ç›‘æ§")
                                yield f"data: [ä»»åŠ¡å·²å®Œæˆ: {status}]\n\n"
                                break

                            # è®¡æ•°æ— æ•°æ®æ¬¡æ•°
                            no_data_count += 1
                            if no_data_count >= max_no_data_iterations:
                                logger.warning("â±ï¸ é•¿æ—¶é—´æ— æ–°æ—¥å¿—ï¼Œè‡ªåŠ¨åœæ­¢ç›‘æ§")
                                yield "data: [é•¿æ—¶é—´æ— æ–°æ—¥å¿—ï¼Œè‡ªåŠ¨åœæ­¢ç›‘æ§]\n\n"
                                break

                            # ç­‰å¾…æ–°æ•°æ®
                            await asyncio.sleep(LOG_STREAM_INTERVAL)

                logger.info("ğŸ æ—¥å¿—æµä¼ è¾“ç»“æŸ")

            except asyncio.CancelledError:
                logger.info("âŒ å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼Œåœæ­¢æ—¥å¿—æµ")
                raise
            except Exception as e:
                logger.error(f"âŒ æ—¥å¿—æµä¼ è¾“é”™è¯¯: {str(e)}", exc_info=True)
                yield f"data: [é”™è¯¯: {str(e)}]\n\n"

        return StreamingResponse(
            log_streamer(),
            media_type="text/event-stream",  # ä½¿ç”¨ SSE æ ¼å¼
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",  # ç¦ç”¨ nginx ç¼“å†²
                "Connection": "keep-alive"
            }
        )
    else:
        # ===== æ™®é€šæ¨¡å¼ï¼šä¸€æ¬¡æ€§è¿”å›å…¨éƒ¨æ—¥å¿— =====
        try:
            file_size = log_path.stat().st_size

            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œè¿‡å¤§æ—¶ç»™å‡ºè­¦å‘Š
            if file_size > 10 * 1024 * 1024:  # 10MB
                logger.warning(f"âš ï¸ æ—¥å¿—æ–‡ä»¶è¾ƒå¤§: {file_size / 1024 / 1024:.2f} MB")

            with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            logger.info(f"âœ… æˆåŠŸè¯»å–æ—¥å¿—ï¼Œå¤§å°: {len(content)} å­—ç¬¦")

            return JSONResponse(
                content={
                    "task_id": task_id,
                    "logs": content,
                    "size": file_size,
                    "lines": content.count('\n')
                }
            )

        except Exception as e:
            logger.error(f"âŒ è¯»å–æ—¥å¿—å¤±è´¥: {str(e)}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"è¯»å–æ—¥å¿—å¤±è´¥: {str(e)}"
            )


# @router.get(
#     "/{task_id}/logs/tail",
#     summary="è·å–æœ€åNè¡Œæ—¥å¿—",
#     description="ä½¿ç”¨ `tail` å‘½ä»¤è·å–æ—¥å¿—çš„æœ€åè‹¥å¹²è¡Œ",
#     response_description="è¿”å›æœ€åNè¡Œæ—¥å¿—å†…å®¹"
# )
async def get_task_logs_tail(
        task_id: str,
        lines: int = Query(
            DEFAULT_TAIL_LINES,
            ge=1,
            le=MAX_TAIL_LINES,
            description=f"è¦è·å–çš„è¡Œæ•° (1-{MAX_TAIL_LINES})"
        )
):
    """
    ## ğŸ“œ è·å–æœ€åNè¡Œæ—¥å¿—

    ### åŠŸèƒ½è¯´æ˜
    é«˜æ•ˆè·å–æ—¥å¿—æ–‡ä»¶çš„æœ€åè‹¥å¹²è¡Œï¼Œé€‚ç”¨äºå¿«é€ŸæŸ¥çœ‹æœ€æ–°æ—¥å¿—ã€‚

    ### å‚æ•°
    - `task_id`: ä»»åŠ¡ID
    - `lines`: è¦è·å–çš„è¡Œæ•°ï¼ˆé»˜è®¤ 100 è¡Œï¼‰

    ### è¿”å›å€¼
    JSON æ ¼å¼ï¼ŒåŒ…å«æœ€åNè¡Œæ—¥å¿—

    ### ç¤ºä¾‹
    ```bash
    # è·å–æœ€å100è¡Œ
    GET /api/tasks/{task_id}/logs/tail

    # è·å–æœ€å500è¡Œ
    GET /api/tasks/{task_id}/logs/tail?lines=500
    ```
    """
    log_path = get_log_path(task_id)
    logger.info(f"ğŸ“œ è·å–æœ€å {lines} è¡Œæ—¥å¿—: {log_path}")

    try:
        # ä½¿ç”¨ tail å‘½ä»¤ï¼ˆæ›´é«˜æ•ˆï¼‰
        result = subprocess.run(
            ['tail', '-n', str(lines), str(log_path)],
            capture_output=True,
            text=True,
            timeout=10,  # 10ç§’è¶…æ—¶
            encoding='utf-8',
            errors='ignore'
        )

        if result.returncode != 0:
            logger.error(f"âŒ tail å‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}")
            raise HTTPException(
                status_code=500,
                detail=f"è·å–æ—¥å¿—å¤±è´¥: {result.stderr}"
            )

        log_content = result.stdout
        actual_lines = log_content.count('\n')

        logger.info(f"âœ… æˆåŠŸè·å– {actual_lines} è¡Œæ—¥å¿—")

        return JSONResponse(
            content={
                "task_id": task_id,
                "logs": log_content,
                "requested_lines": lines,
                "actual_lines": actual_lines
            }
        )

    except subprocess.TimeoutExpired:
        logger.error("âŒ tail å‘½ä»¤æ‰§è¡Œè¶…æ—¶")
        raise HTTPException(
            status_code=500,
            detail="è·å–æ—¥å¿—è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        )
    except Exception as e:
        logger.error(f"âŒ è·å–æ—¥å¿—å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ—¥å¿—å¤±è´¥: {str(e)}"
        )


# @router.get(
#     "/{task_id}/logs/download",
#     summary="ä¸‹è½½æ—¥å¿—æ–‡ä»¶",
#     description="ä¸‹è½½å®Œæ•´çš„æ—¥å¿—æ–‡ä»¶",
#     response_class=StreamingResponse
# )
async def download_task_logs(task_id: str):
    """
    ## ğŸ’¾ ä¸‹è½½æ—¥å¿—æ–‡ä»¶

    ### åŠŸèƒ½è¯´æ˜
    ä¸‹è½½å®Œæ•´çš„æ—¥å¿—æ–‡ä»¶åˆ°æœ¬åœ°

    ### å‚æ•°
    - `task_id`: ä»»åŠ¡ID

    ### è¿”å›å€¼
    æ–‡ä»¶æµï¼ˆapplication/octet-streamï¼‰

    ### ç¤ºä¾‹
    ```bash
    GET /api/tasks/{task_id}/logs/download
    ```
    """
    log_path = get_log_path(task_id)
    logger.info(f"ğŸ’¾ ä¸‹è½½æ—¥å¿—æ–‡ä»¶: {log_path}")

    try:
        def file_iterator():
            """æ–‡ä»¶è¿­ä»£å™¨"""
            with open(log_path, 'rb') as f:
                while chunk := f.read(8192):  # 8KB æ¯æ¬¡
                    yield chunk

        file_size = log_path.stat().st_size

        return StreamingResponse(
            file_iterator(),
            media_type="application/octet-stream",
            headers={
                "Content-Disposition": f'attachment; filename="task_{task_id}.log"',
                "Content-Length": str(file_size)
            }
        )
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½æ—¥å¿—å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"ä¸‹è½½æ—¥å¿—å¤±è´¥: {str(e)}"
        )


# @router.delete(
#     "/{task_id}/logs",
#     summary="ğŸ—‘åˆ é™¤æ—¥å¿—æ–‡ä»¶",
#     description="åˆ é™¤æŒ‡å®šä»»åŠ¡çš„æ—¥å¿—æ–‡ä»¶"
# )
async def delete_task_logs(task_id: str):
    """
    ## ğŸ—‘ï¸ åˆ é™¤æ—¥å¿—æ–‡ä»¶

    ### åŠŸèƒ½è¯´æ˜
    åˆ é™¤æŒ‡å®šä»»åŠ¡çš„æ—¥å¿—æ–‡ä»¶ï¼ˆè°¨æ…æ“ä½œï¼‰

    ### å‚æ•°
    - `task_id`: ä»»åŠ¡ID

    ### è¿”å›å€¼
    åˆ é™¤ç»“æœ

    ### ç¤ºä¾‹
    ```bash
    DELETE /api/tasks/{task_id}/logs
    ```
    """
    log_path = get_log_path(task_id)

    try:
        log_path.unlink()
        logger.info(f"ğŸ—‘ï¸ æˆåŠŸåˆ é™¤æ—¥å¿—æ–‡ä»¶: {log_path}")

        return JSONResponse(
            content={
                "message": "æ—¥å¿—æ–‡ä»¶å·²åˆ é™¤",
                "task_id": task_id,
                "path": str(log_path)
            }
        )
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤æ—¥å¿—å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"åˆ é™¤æ—¥å¿—å¤±è´¥: {str(e)}"
        )